[{"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 34, "SoC": 687, "makespan": 34, "avg_agents_density": 0.09754076597506085, "runtime": 0.864059404033469}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-000"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 47, "SoC": 853, "makespan": 47, "avg_agents_density": 0.12029706015516217, "runtime": 0.8345217650057748}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-001"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.90625, "CSR": 0.0, "ep_length": 128, "SoC": 1203, "makespan": 128, "avg_agents_density": 0.0814034716396396, "runtime": 0.8485382530343486}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-002"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 60, "SoC": 855, "makespan": 60, "avg_agents_density": 0.104132939146312, "runtime": 1.4272291659290204}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-003"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 58, "SoC": 1036, "makespan": 58, "avg_agents_density": 0.11073517819419212, "runtime": 0.9680019779625582}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-004"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 68, "SoC": 1429, "makespan": 68, "avg_agents_density": 0.11856190514983518, "runtime": 1.5121935149509227}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-005"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 35, "SoC": 660, "makespan": 35, "avg_agents_density": 0.09854389661683909, "runtime": 0.776198648076388}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-006"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 1671, "makespan": 128, "avg_agents_density": 0.10262312391331539, "runtime": 3.1901733570266515}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-007"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 54, "SoC": 1133, "makespan": 54, "avg_agents_density": 0.10978508462617362, "runtime": 1.2654457649769029}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-008"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 43, "SoC": 790, "makespan": 43, "avg_agents_density": 0.1310839483676907, "runtime": 0.919225000005099}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-009"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 59, "SoC": 982, "makespan": 59, "avg_agents_density": 0.08927472440607312, "runtime": 1.4239425760315498}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-010"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 1492, "makespan": 128, "avg_agents_density": 0.12991449929535465, "runtime": 2.042111481016036}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-011"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 48, "SoC": 832, "makespan": 48, "avg_agents_density": 0.10835984956636877, "runtime": 1.2813271670165705}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-012"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 46, "SoC": 744, "makespan": 46, "avg_agents_density": 0.10680640046336601, "runtime": 1.1322607050533406}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-013"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 46, "SoC": 809, "makespan": 46, "avg_agents_density": 0.12101273359665649, "runtime": 1.06022799591301}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-014"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 68, "SoC": 1239, "makespan": 68, "avg_agents_density": 0.11167240986328975, "runtime": 1.0359346571494825}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-015"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 95, "SoC": 1306, "makespan": 95, "avg_agents_density": 0.10501333164027497, "runtime": 2.1584854849788826}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-016"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 50, "SoC": 749, "makespan": 50, "avg_agents_density": 0.08866523505367899, "runtime": 0.5118043960392242}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-017"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 1599, "makespan": 128, "avg_agents_density": 0.11591443624828959, "runtime": 2.98168273299234}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-018"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 67, "SoC": 1431, "makespan": 67, "avg_agents_density": 0.1302078440257224, "runtime": 0.1935557379911188}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-019"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 1161, "makespan": 128, "avg_agents_density": 0.09421086285897601, "runtime": 1.2880562180944253}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-020"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 47, "SoC": 1053, "makespan": 47, "avg_agents_density": 0.12014307489654717, "runtime": 0.43042678001802415}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-021"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 39, "SoC": 709, "makespan": 39, "avg_agents_density": 0.09736435089094858, "runtime": 0.6541155570012052}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-022"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 62, "SoC": 960, "makespan": 62, "avg_agents_density": 0.10779462448998266, "runtime": 1.449464435048867}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-023"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 57, "SoC": 982, "makespan": 57, "avg_agents_density": 0.10205433938677853, "runtime": 1.21396093799558}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-024"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 31, "SoC": 440, "makespan": 31, "avg_agents_density": 0.09519181552677636, "runtime": 0.5556969660246978}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-025"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 48, "SoC": 734, "makespan": 48, "avg_agents_density": 0.0932418468281162, "runtime": 0.4433701900416054}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-026"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 58, "SoC": 1006, "makespan": 58, "avg_agents_density": 0.08628822012370928, "runtime": 0.9948421589942882}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-027"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 87, "SoC": 1263, "makespan": 87, "avg_agents_density": 0.09435858123233665, "runtime": 0.24656565600889735}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-028"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 41, "SoC": 898, "makespan": 41, "avg_agents_density": 0.11939189713530819, "runtime": 0.23839035593846347}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-029"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 47, "SoC": 849, "makespan": 47, "avg_agents_density": 0.11321818051669807, "runtime": 0.378859043106786}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-030"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 35, "SoC": 567, "makespan": 35, "avg_agents_density": 0.1087217621919375, "runtime": 0.44702004399732687}, "env_grid_search": {"num_agents": 32, "map_name": "validation-mazes-seed-031"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 58, "SoC": 1786, "makespan": 58, "avg_agents_density": 0.15135579653130776, "runtime": 1.8026185419876128}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-000"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 67, "SoC": 2073, "makespan": 67, "avg_agents_density": 0.1595981376342725, "runtime": 1.6869623369711917}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-001"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 99, "SoC": 1782, "makespan": 99, "avg_agents_density": 0.13440949363280733, "runtime": 2.453131964066415}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-002"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 61, "SoC": 1853, "makespan": 61, "avg_agents_density": 0.16762760100961335, "runtime": 1.4226725670014275}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-003"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 64, "SoC": 1805, "makespan": 64, "avg_agents_density": 0.15474984824670787, "runtime": 1.7663385680207284}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-004"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9166666666666666, "CSR": 0.0, "ep_length": 128, "SoC": 3169, "makespan": 128, "avg_agents_density": 0.1560112334093133, "runtime": 3.1540497509995475}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-005"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 50, "SoC": 1392, "makespan": 50, "avg_agents_density": 0.13907150232347246, "runtime": 1.2613606360100675}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-006"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.7708333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 4679, "makespan": 128, "avg_agents_density": 0.1903523429434983, "runtime": 3.6279587779717986}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-007"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 86, "SoC": 2230, "makespan": 86, "avg_agents_density": 0.16308648549658225, "runtime": 2.2238408529083245}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-008"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 84, "SoC": 2026, "makespan": 84, "avg_agents_density": 0.17601611498483968, "runtime": 2.369517639061087}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-009"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 63, "SoC": 1597, "makespan": 63, "avg_agents_density": 0.12884473153020193, "runtime": 1.6855525030259741}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-010"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.5833333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 5301, "makespan": 128, "avg_agents_density": 0.2792918673462089, "runtime": 3.394688378102728}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-011"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 61, "SoC": 1965, "makespan": 61, "avg_agents_density": 0.16877799951052336, "runtime": 2.036902941952576}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-012"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9583333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 1767, "makespan": 128, "avg_agents_density": 0.15123350151341228, "runtime": 3.6127818821405526}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-013"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 50, "SoC": 1481, "makespan": 50, "avg_agents_density": 0.16989413752856564, "runtime": 1.1875539609318366}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-014"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 75, "SoC": 2299, "makespan": 75, "avg_agents_density": 0.16339853221702286, "runtime": 2.4540294819889823}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-015"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 67, "SoC": 2051, "makespan": 67, "avg_agents_density": 0.16505321323277145, "runtime": 1.5539529520610813}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-016"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 57, "SoC": 1426, "makespan": 57, "avg_agents_density": 0.12210863451029265, "runtime": 1.4652325709466822}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-017"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9583333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 3437, "makespan": 128, "avg_agents_density": 0.160381126063464, "runtime": 3.4080176419811323}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-018"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.8333333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 5071, "makespan": 128, "avg_agents_density": 0.20839192439928314, "runtime": 2.909586914043757}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-019"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 107, "SoC": 2169, "makespan": 107, "avg_agents_density": 0.14846558165936252, "runtime": 2.388406682977802}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-020"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9791666666666666, "CSR": 0.0, "ep_length": 128, "SoC": 2360, "makespan": 128, "avg_agents_density": 0.15886029078176123, "runtime": 2.9442589009267977}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-021"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 58, "SoC": 1394, "makespan": 58, "avg_agents_density": 0.16350948496511938, "runtime": 1.362428208012716}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-022"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 47, "SoC": 1613, "makespan": 47, "avg_agents_density": 0.17528029645070106, "runtime": 1.1091953489958541}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-023"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9791666666666666, "CSR": 0.0, "ep_length": 128, "SoC": 2539, "makespan": 128, "avg_agents_density": 0.1527014665287743, "runtime": 2.843006676994264}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-024"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 33, "SoC": 715, "makespan": 33, "avg_agents_density": 0.13705130882761263, "runtime": 0.8769098350167042}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-025"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 43, "SoC": 1174, "makespan": 43, "avg_agents_density": 0.13062965245740124, "runtime": 1.4240864849998616}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-026"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 106, "SoC": 2083, "makespan": 106, "avg_agents_density": 0.13406051267441765, "runtime": 2.628850638997392}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-027"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9583333333333334, "CSR": 0.0, "ep_length": 128, "SoC": 3453, "makespan": 128, "avg_agents_density": 0.14537159890834883, "runtime": 3.190399468963733}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-028"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 85, "SoC": 3043, "makespan": 85, "avg_agents_density": 0.17906795472394058, "runtime": 2.1742795659665717}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-029"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 80, "SoC": 1877, "makespan": 80, "avg_agents_density": 0.18058134543487728, "runtime": 2.044771874032449}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-030"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 42, "SoC": 960, "makespan": 42, "avg_agents_density": 0.14689939520253376, "runtime": 1.1868052339850692}, "env_grid_search": {"num_agents": 48, "map_name": "validation-mazes-seed-031"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 94, "SoC": 3932, "makespan": 94, "avg_agents_density": 0.20462982982028574, "runtime": 3.9058864419785095}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-000"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 4944, "makespan": 128, "avg_agents_density": 0.20796694852413486, "runtime": 4.854937825963134}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-001"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 4469, "makespan": 128, "avg_agents_density": 0.20001667784789545, "runtime": 5.223821604042314}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-002"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 94, "SoC": 3142, "makespan": 94, "avg_agents_density": 0.2029777558139362, "runtime": 3.8839090499968734}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-003"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.59375, "CSR": 0.0, "ep_length": 128, "SoC": 4802, "makespan": 128, "avg_agents_density": 0.2743785796296493, "runtime": 4.901335793940234}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-004"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.765625, "CSR": 0.0, "ep_length": 128, "SoC": 6549, "makespan": 128, "avg_agents_density": 0.21823120858616338, "runtime": 4.847271463033394}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-005"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.9375, "CSR": 0.0, "ep_length": 128, "SoC": 3624, "makespan": 128, "avg_agents_density": 0.18874783928765743, "runtime": 4.421252832908067}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-006"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.390625, "CSR": 0.0, "ep_length": 128, "SoC": 6922, "makespan": 128, "avg_agents_density": 0.27879809887587764, "runtime": 4.158636378968367}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-007"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 124, "SoC": 4879, "makespan": 124, "avg_agents_density": 0.21346744336468232, "runtime": 4.0117512730066665}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-008"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 105, "SoC": 3946, "makespan": 105, "avg_agents_density": 0.22218628377277239, "runtime": 3.49516442703316}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-009"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.734375, "CSR": 0.0, "ep_length": 128, "SoC": 4508, "makespan": 128, "avg_agents_density": 0.18086357029640304, "runtime": 4.1782031349721365}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-010"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.28125, "CSR": 0.0, "ep_length": 128, "SoC": 7017, "makespan": 128, "avg_agents_density": 0.3857098098514618, "runtime": 4.270591600041371}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-011"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 121, "SoC": 3842, "makespan": 121, "avg_agents_density": 0.20641646064627422, "runtime": 3.9900913449382642}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-012"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 84, "SoC": 3205, "makespan": 84, "avg_agents_density": 0.19539725421817764, "runtime": 2.7379888679715805}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-013"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 128, "SoC": 3878, "makespan": 128, "avg_agents_density": 0.2062496005963558, "runtime": 4.954901608012733}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-014"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.96875, "CSR": 0.0, "ep_length": 128, "SoC": 4110, "makespan": 128, "avg_agents_density": 0.2081620121717083, "runtime": 4.043585952982539}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-015"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.921875, "CSR": 0.0, "ep_length": 128, "SoC": 6393, "makespan": 128, "avg_agents_density": 0.2267018877087974, "runtime": 4.073704268070287}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-016"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 62, "SoC": 2320, "makespan": 62, "avg_agents_density": 0.15647078292358507, "runtime": 1.9922744050127221}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-017"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.875, "CSR": 0.0, "ep_length": 128, "SoC": 5485, "makespan": 128, "avg_agents_density": 0.1976048321596098, "runtime": 4.130780837032944}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-018"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.5, "CSR": 0.0, "ep_length": 128, "SoC": 6918, "makespan": 128, "avg_agents_density": 0.27786864099565284, "runtime": 4.124078171109431}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-019"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.921875, "CSR": 0.0, "ep_length": 128, "SoC": 4643, "makespan": 128, "avg_agents_density": 0.20148093615518398, "runtime": 4.109737776016118}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-020"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.984375, "CSR": 0.0, "ep_length": 128, "SoC": 4690, "makespan": 128, "avg_agents_density": 0.21415082784391082, "runtime": 4.147908059167094}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-021"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 65, "SoC": 2015, "makespan": 65, "avg_agents_density": 0.19194246858537717, "runtime": 2.0541086409648415}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-022"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.984375, "CSR": 0.0, "ep_length": 128, "SoC": 5090, "makespan": 128, "avg_agents_density": 0.20211904278383966, "runtime": 4.025479546908173}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-023"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 121, "SoC": 4313, "makespan": 121, "avg_agents_density": 0.19421088179284615, "runtime": 3.885978580932715}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-024"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 36, "SoC": 1079, "makespan": 36, "avg_agents_density": 0.18435993105013923, "runtime": 1.1536541590030538}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-025"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 67, "SoC": 2498, "makespan": 67, "avg_agents_density": 0.17562495651176502, "runtime": 2.2357595440698788}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-026"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.890625, "CSR": 0.0, "ep_length": 128, "SoC": 5011, "makespan": 128, "avg_agents_density": 0.1890344304605679, "runtime": 4.01562270600698}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-027"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.828125, "CSR": 0.0, "ep_length": 128, "SoC": 6891, "makespan": 128, "avg_agents_density": 0.228951107753466, "runtime": 4.062690487946384}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-028"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 115, "SoC": 5252, "makespan": 115, "avg_agents_density": 0.2316235178228094, "runtime": 3.6410467259411234}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-029"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 0.890625, "CSR": 0.0, "ep_length": 128, "SoC": 5227, "makespan": 128, "avg_agents_density": 0.218320802339268, "runtime": 4.209310710066347}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-030"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}, {"metrics": {"ISR": 1.0, "CSR": 1.0, "ep_length": 73, "SoC": 2362, "makespan": 73, "avg_agents_density": 0.19743265249177513, "runtime": 2.961881031951634}, "env_grid_search": {"num_agents": 64, "map_name": "validation-mazes-seed-031"}, "algorithm": "MAPF-GPT-2M-dagger-16000"}]